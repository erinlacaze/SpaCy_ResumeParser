{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/dudegladiator/ResumeParser"
      ],
      "metadata": {
        "id": "Qcx6sNOqLXv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "soo8udjXAbOe"
      },
      "outputs": [],
      "source": [
        "!pip install spacy==3.7.2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.tokens import DocBin\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import json\n",
        "import spacy\n",
        "from spacy.util import filter_spans\n",
        "nlp = spacy.blank(\"en\") # load a new spacy model\n",
        "doc_bin = DocBin()\n",
        "\n",
        "\n",
        "!python -m spacy download en_core_web_lg"
      ],
      "metadata": {
        "id": "rzokUOIqAmDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###Train2\n",
        "\n",
        "def trim_entity_spans(data: list) -> list:\n",
        "\n",
        "    invalid_span_tokens = re.compile(r'\\s')\n",
        "\n",
        "    cleaned_data = []\n",
        "    for text, annotations in data:\n",
        "        entities = annotations['entities']\n",
        "        valid_entities = []\n",
        "        for start, end, label in entities:\n",
        "            valid_start = start\n",
        "            valid_end = end\n",
        "            while valid_start < len(text) and invalid_span_tokens.match(\n",
        "                    text[valid_start]):\n",
        "                valid_start += 1\n",
        "            while valid_end > 1 and invalid_span_tokens.match(\n",
        "                    text[valid_end - 1]):\n",
        "                valid_end -= 1\n",
        "            valid_entities.append([valid_start, valid_end, label])\n",
        "        cleaned_data.append([text, {'entities': valid_entities}])\n",
        "\n",
        "    return cleaned_data\n",
        "\n",
        "\n",
        "def convert_dataturks_to_spacy(dataturks_JSON_FilePath):\n",
        "    try:\n",
        "        training_data = []\n",
        "        lines = []\n",
        "        with open(dataturks_JSON_FilePath, 'r', encoding=\"utf8\") as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        for line in lines:\n",
        "            data = json.loads(line)\n",
        "            text = data['content']\n",
        "            entities = []\n",
        "            if data['annotation'] is not None:\n",
        "                for annotation in data['annotation']:\n",
        "                    # only a single point in text annotation.\n",
        "                    point = annotation['points'][0]\n",
        "                    labels = annotation['label']\n",
        "                    # handle both list of labels or a single label.\n",
        "                    if not isinstance(labels, list):\n",
        "                        labels = [labels]\n",
        "\n",
        "                    for label in labels:\n",
        "                        # dataturks indices are both inclusive [start, end]\n",
        "                        # but spacy is not [start, end)\n",
        "                        entities.append((\n",
        "                            point['start'],\n",
        "                            point['end'] + 1,\n",
        "                            label\n",
        "                        ))\n",
        "\n",
        "            training_data.append((text, {\"entities\": entities}))\n",
        "        return training_data\n",
        "    except Exception as e :\n",
        "\n",
        "        logging.exception(\"Unable to process \" + dataturks_JSON_FilePath)\n",
        "        return None\n",
        "\n",
        "# start with afreen dataset\n",
        "TRAIN_DATA = trim_entity_spans(convert_dataturks_to_spacy(\"/content/model2_dataset.json\"))\n",
        "\n"
      ],
      "metadata": {
        "id": "0MhRjisDAl44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data = []\n",
        "for example in TRAIN_DATA:\n",
        "  temp_dict = {}\n",
        "  temp_dict['text'] = example[0]\n",
        "  temp_dict['entities'] = []\n",
        "  range1 = []\n",
        "  for annotation in example[1][\"entities\"]:\n",
        "    start = annotation[0]\n",
        "    end = annotation[1]\n",
        "    label = annotation[2].upper()\n",
        "    skip = False\n",
        "    for idx in range(start,end):\n",
        "      if idx in range1:\n",
        "        print(\"more --\" ,idx)\n",
        "        skip = True\n",
        "        break\n",
        "    if skip or start>=end:\n",
        "          continue\n",
        "    range1 += list(range(start,end))\n",
        "    temp_dict['entities'].append((start, end, label))\n",
        "  training_data.append(temp_dict)\n",
        "\n",
        "print(training_data[0])"
      ],
      "metadata": {
        "id": "l3kxlgk0BVHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count=0\n",
        "for training_example  in tqdm(training_data):\n",
        "    text = training_example['text']\n",
        "    labels = training_example['entities']\n",
        "    doc = nlp.make_doc(text)\n",
        "    ents = []\n",
        "    for start, end, label in labels:\n",
        "        span = doc.char_span(start, end, label=label, alignment_mode=\"expand\")\n",
        "        if span is None:\n",
        "            print(\"Skipping entity\")\n",
        "\n",
        "        else:\n",
        "            ents.append(span)\n",
        "    count+=1\n",
        "    filtered_ents = filter_spans(ents)\n",
        "    doc.ents = filtered_ents\n",
        "    doc_bin.add(doc)\n",
        "print(count)\n",
        "\n",
        "doc_bin.to_disk(\"model2.spacy\")"
      ],
      "metadata": {
        "id": "wLhOe91fBUZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please get the base config file from spacy website"
      ],
      "metadata": {
        "id": "OELeJWfh_ijp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy init fill-config /kaggle/input/base-config/base_config.cfg config.cfg"
      ],
      "metadata": {
        "id": "SS3JAEDPBZwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you get the config file, then directly run below code\n",
        "\n",
        "---\n",
        "\n",
        "Upload the config file to root\n"
      ],
      "metadata": {
        "id": "-hKM6R_8_nxb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy train /kaggle/input/config/config.cfg --output ./output2 --paths.train ./model2.spacy --paths.dev ./model2.spacy"
      ],
      "metadata": {
        "id": "R1TTIQ1XBZuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "go to the ouput2 folder, model-best is model2"
      ],
      "metadata": {
        "id": "myQUmVGaBklT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = spacy.load(\"/content/output2/model-best\")"
      ],
      "metadata": {
        "id": "lMdVly-RDJUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2.get_pipe(\"ner\").labels"
      ],
      "metadata": {
        "id": "XUtBzCcdDs9q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
